{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4GlL84PfVaH"
      },
      "source": [
        "Word Embedding is a language modeling technique used for mapping words to vectors of real numbers. It represents words or phrases in vector space with several dimensions. Word embeddings can be generated using various methods like neural networks, co-occurrence matrix, probabilistic models, etc. Word2Vec consists of models for generating word embedding. \n",
        "\n",
        "In natural language processing (NLP), a word embedding is a representation of a word. The embedding is used in text analysis. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that words that are closer in the vector space are expected to be similar in meaning.\n",
        "\n",
        "What is word embedding with example?\n",
        "\n",
        "Thus by using word embeddings, words that are close in meaning are grouped near to one another in vector space. For example, while representing a word such as frog, the nearest neighbour of a frog would be frogs, toads, Litoria.\n",
        "\n",
        "Applications of Word Embedding :\n",
        "\n",
        "1. Sentiment Analysis\n",
        "2. Speech Recognition\n",
        "3. Information Retrieval\n",
        "4. Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A9g7oD3dSF_w"
      },
      "outputs": [],
      "source": [
        "#define tokenized_sentences as traning data\n",
        "tokenized_sentences=[['Hello','this','is','Python','traning','by','vardhaman'],\n",
        "                    ['Hello','this','is','Java','traning','by','vardhaman'],\n",
        "                    ['Hello','this','is','Data Science','traning','by','unfold','Data','science'],\n",
        "                    ['Hello','this','is','programming','traning','']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eMryF8-gTjru"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec<vocab=14, vector_size=100, alpha=0.025>\n"
          ]
        }
      ],
      "source": [
        "# pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(tokenized_sentences,min_count=1) # traning word2vec model\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iNYDfJUVYpg",
        "outputId": "d314fcac-05d7-47de-baf9-c1d12f607866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['traning', 'is', 'this', 'Hello', 'by', 'vardhaman', '', 'programming', 'science', 'Data', 'unfold', 'Data Science', 'Java', 'Python'])\n"
          ]
        }
      ],
      "source": [
        "vocab = model.wv.key_to_index.keys() # summarize vocabulary \n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJtmKwq_YCBt",
        "outputId": "43d77f76-0051-4a81-819c-ca4fdef3df1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
            " -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
            " -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
            " -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
            " -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
            " -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
            " -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
            "  1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
            " -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
            "  5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
            "  9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
            " -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
            " -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
            "  7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
            "  9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
            "  8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
            "  3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
            " -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
            "  7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
            " -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
            "  7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
            " -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
            " -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
            " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
            " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]\n"
          ]
        }
      ],
      "source": [
        "print(model.wv['vardhaman'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHTLDFr-clPS",
        "outputId": "10036bc0-6228-4131-f53e-932f42d23742"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Data Science', 0.16695639491081238),\n",
              " ('by', 0.13887985050678253),\n",
              " ('Hello', 0.13149002194404602),\n",
              " ('this', 0.06408979743719101),\n",
              " ('science', 0.0606001578271389),\n",
              " ('Python', 0.044106729328632355),\n",
              " ('unfold', 0.020013250410556793),\n",
              " ('', 0.01915227249264717),\n",
              " ('is', 0.009392211213707924),\n",
              " ('programming', -0.05774582922458649)]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.most_similar('vardhaman') # finding most similar words for word 'traning'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
